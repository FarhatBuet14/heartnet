{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "# %matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import tables\n",
    "from utils import reshape_folds\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.models import model_from_json\n",
    "from custom_layers import Conv1D_linearphaseType, Conv1D_linearphase, DCT1D, \\\n",
    "            Conv1D_gammatone, Conv1D_linearphaseType_legacy, Conv1D_zerophase\n",
    "    \n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from scipy import signal\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "sns.set()\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(model, model_inputs, batch_size=64,print_shape_only=True, layer_name=None):\n",
    "    '''\n",
    "    Get activations from a specific layer of a trained model\n",
    "    '''\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "\n",
    "    model_multi_inputs_cond = True\n",
    "    if not isinstance(inp, list):\n",
    "        # only one input! let's wrap it in a list.\n",
    "        inp = [inp]\n",
    "        model_multi_inputs_cond = False\n",
    "\n",
    "    outputs = [layer.output for layer in model.layers if\n",
    "               layer.name == layer_name or layer_name is None]  # all layer outputs\n",
    "\n",
    "    funcs = [K.function(inp + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    \n",
    "    start_idx = 0\n",
    "    for idx in range(batch_size,len(model_inputs)+batch_size,batch_size):\n",
    "#         print(batch_size)\n",
    "        if model_multi_inputs_cond:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            list_inputs = [model_inputs[start_idx:idx], 0.]\n",
    "\n",
    "        # Learning phase. 0 = Test mode (no dropout or batch normalization)\n",
    "        # layer_outputs = [func([model_inputs, 0.])[0] for func in funcs]\n",
    "        layer_outputs = [func(list_inputs)[0] for func in funcs]\n",
    "        for layer_activations in layer_outputs:\n",
    "            activations.append(layer_activations)\n",
    "        start_idx = idx\n",
    "    return np.vstack(activations)\n",
    "\n",
    "\n",
    "def display_activations(activation_maps):\n",
    "    '''\n",
    "    Plot activations\n",
    "    '''\n",
    "    batch_size = activation_maps[0].shape[0]\n",
    "    assert batch_size == 1, 'One image at a time to visualize.'\n",
    "    for i, activation_map in enumerate(activation_maps):\n",
    "        print('Displaying activation map {}'.format(i))\n",
    "        shape = activation_map.shape\n",
    "        if len(shape) == 4:\n",
    "            activations = np.hstack(np.transpose(activation_map[0], (2, 0, 1)))\n",
    "        elif len(shape) == 2:\n",
    "            # try to make it square as much as possible. we can skip some activations.\n",
    "            activations = activation_map[0]\n",
    "            num_activations = len(activations)\n",
    "            if num_activations > 1024:  # too hard to display it on the screen.\n",
    "                square_param = int(np.floor(np.sqrt(num_activations)))\n",
    "                activations = activations[0: square_param * square_param]\n",
    "                activations = np.reshape(activations, (square_param, square_param))\n",
    "            else:\n",
    "                activations = np.expand_dims(activations, axis=0)\n",
    "        else:\n",
    "            raise Exception('len(shape) = 3 has not been implemented.')\n",
    "        plt.imshow(activations, interpolation='None', cmap='jet')\n",
    "    plt.show()\n",
    "\n",
    "def smooth(scalars, weight):  # Weight between 0 and 1\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "\n",
    "    return np.asarray(smoothed)\n",
    "    \n",
    "def get_weights(log_name,min_metric=.7,min_epoch=50,verbose=1,log_dir='../logs'):\n",
    "    '''\n",
    "    Load weights from training.csv file\n",
    "    '''\n",
    "    log_dir = '../logs'\n",
    "    \n",
    "    if not os.path.isdir(os.path.join(log_dir,log_name)):\n",
    "        print(\"Looking into logArxiv\")\n",
    "        log_dir = '/media/mhealthra2/Data/heart_sound/logArxiv'\n",
    "    training_csv = os.path.join(log_dir,log_name,\"training.csv\")\n",
    "    df = pd.read_csv(training_csv)\n",
    "    sens_idx = df['val_sensitivity'][df.epoch>min_epoch][df.val_specificity>min_metric].idxmax()\n",
    "    spec_idx = df['val_specificity'][df.epoch>min_epoch][df.val_sensitivity>min_metric].idxmax()\n",
    "    macc_idx = df['val_macc'][df.epoch>min_epoch].idxmax()\n",
    "    val_idx = df['val_acc'][df.epoch>min_epoch].idxmax()\n",
    "    weights = dict()\n",
    "    weights['val_sensitivity'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[sens_idx]+1,df.val_acc.iloc[sens_idx])\n",
    "    weights['val_specificity'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[spec_idx]+1,df.val_acc.iloc[spec_idx])\n",
    "    weights['val_macc'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[macc_idx]+1,df.val_acc.iloc[macc_idx])\n",
    "    weights['val_acc'] = \"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[val_idx]+1,df.val_acc.iloc[val_idx])\n",
    "    weights['epoch'] = [\"weights.%.4d-%.4f.hdf5\" % (df.epoch.iloc[idx]+1,df.val_acc.iloc[idx]) \n",
    "                        for idx in range(df.count()[0])]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Best Sensitivity model: {} \\t\\t{}\".format(df.val_sensitivity.iloc[sens_idx],weights['val_sensitivity']))\n",
    "        print(\"Best Specificity model: {} \\t\\t{}\".format(df.val_specificity.iloc[spec_idx],weights['val_specificity']))\n",
    "        print(\"Best Macc model: {} \\t\\t{}\".format(df.val_macc.iloc[macc_idx],weights['val_macc']))\n",
    "        print(\"Best Val model: {} \\t\\t\\t{}\".format(df.val_acc.iloc[val_idx],weights['val_acc']))\n",
    "    return weights\n",
    "\n",
    "      \n",
    "def load_data(foldname,fold_dir=None,_categorical=True,quality=False):\n",
    "    ## import data\n",
    "    if fold_dir is None:\n",
    "        fold_dir = '/media/mhealthra2/Data/heart_sound/feature/segmented_noFIR/folds_dec_2018/'\n",
    "    else:\n",
    "        print(fold_dir+foldname)\n",
    "    feat = tables.open_file(fold_dir + foldname + '.mat')\n",
    "    x_train = feat.root.trainX[:]\n",
    "    y_train = feat.root.trainY[0, :]\n",
    "    q_train = feat.root.trainY[1, :]\n",
    "    x_val = feat.root.valX[:]\n",
    "    y_val = feat.root.valY[0, :]\n",
    "    q_val = feat.root.valY[1, :]\n",
    "    train_parts = feat.root.train_parts[:]\n",
    "    val_parts = feat.root.val_parts[0, :]\n",
    "\n",
    "    ############## Relabeling ################\n",
    "    \n",
    "    for i in range(0, y_train.shape[0]):\n",
    "        if y_train[i] == -1:\n",
    "            y_train[i] = 0  ## Label 0 for normal 1 for abnormal\n",
    "    for i in range(0, y_val.shape[0]):\n",
    "        if y_val[i] == -1:\n",
    "            y_val[i] = 0\n",
    "\n",
    "    ############# Parse Database names ########\n",
    "\n",
    "    train_files = []\n",
    "    for each in feat.root.train_files[:][0]:\n",
    "        train_files.append(chr(each))\n",
    "    print(len(train_files))\n",
    "    val_files = []\n",
    "    for each in feat.root.val_files[:][0]:\n",
    "        val_files.append(chr(each))\n",
    "    print(len(val_files))\n",
    "\n",
    "    ################### Reshaping ############\n",
    "\n",
    "    x_train, y_train, x_val, y_val = reshape_folds(x_train, x_val, y_train, y_val)\n",
    "\n",
    "    if _categorical:\n",
    "        y_train = to_categorical(y_train, num_classes=2)\n",
    "        y_val = to_categorical(y_val, num_classes=2)\n",
    "    \n",
    "    if quality:\n",
    "        return x_train, y_train, train_files, train_parts, q_train, \\\n",
    "                x_val, y_val, val_files, val_parts, q_val\n",
    "    else:\n",
    "        return x_train, y_train, train_files, train_parts, \\\n",
    "                x_val, y_val, val_files, val_parts\n",
    "\n",
    "def load_model(log_name,verbose=0,\n",
    "               model_dir='../models/',\n",
    "               log_dir='../logs/'):\n",
    "    \n",
    "#     model_dir = '/media/mhealthra2/Data/heart_sound/models/'\n",
    "#     log_dir = '/media/mhealthra2/Data/heart_sound/logs/'\n",
    "\n",
    "    if os.path.isdir(model_dir+log_name):\n",
    "        print(\"Model directory found\")\n",
    "        if os.path.isfile(os.path.join(model_dir+log_name,\"model.json\")):\n",
    "            print(\"model.json found. Importing\")\n",
    "        else:\n",
    "            raise ImportError(\"model.json not found\")\n",
    "\n",
    "    with open(os.path.join(model_dir+log_name,\"model.json\")) as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    try:\n",
    "        model = model_from_json(loaded_model_json,{'Conv1D_linearphase':Conv1D_linearphase,\n",
    "                                               'DCT1D':DCT1D,\n",
    "                                               'Conv1D_linearphaseType':Conv1D_linearphaseType,\n",
    "                                               'Conv1D_gammatone' : Conv1D_gammatone,\n",
    "                                                'Conv1D_zerophase' : Conv1D_zerophase,\n",
    "                                              })\n",
    "    except:\n",
    "        model = model_from_json(loaded_model_json,{'Conv1D_linearphase':Conv1D_linearphase,\n",
    "                                               'DCT1D':DCT1D,\n",
    "                                               'Conv1D_linearphaseType':Conv1D_linearphaseType_legacy,\n",
    "                                               'Conv1D_gammatone' : Conv1D_gammatone,\n",
    "                                                   'Conv1D_zerophase' : Conv1D_zerophase,\n",
    "                                              })\n",
    "    if verbose:\n",
    "        print(log_name)\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "def cc2parts(cc,parts):\n",
    "    \n",
    "    if not len(cc) == sum(parts):\n",
    "        raise ValueError('Number of CC elements are not equal to total number of parts')\n",
    "    \n",
    "    labels = []\n",
    "    start_idx = 0\n",
    "#     cc = np.round(cc)\n",
    "    \n",
    "    for s in parts:\n",
    "        if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "            continue\n",
    "        temp = cc[start_idx:start_idx + int(s)]\n",
    "        try:\n",
    "            labels.append(np.mean(temp,axis=0))\n",
    "        except TypeError: ## TypeError for string input in train_files\n",
    "            labels.append(cc[start_idx])\n",
    "        start_idx = start_idx + int(s)\n",
    "    return np.asarray(labels)\n",
    "\n",
    "def parts2cc(partitioned,parts):\n",
    "    \n",
    "    labels = []\n",
    "    parts = parts[np.nonzero(parts)]\n",
    "    for each,part in zip(partitioned,parts):\n",
    "            labels += list(np.repeat(each,part))\n",
    "    return np.asarray(labels)\n",
    "\n",
    "def predict_parts(model,data,labels,parts,filenames=None,verbose=1,soft=False):\n",
    "    y_pred = model.predict(data, verbose=verbose)\n",
    "    true = []\n",
    "    pred = []\n",
    "    files= []\n",
    "    start_idx = 0\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "    y_val = np.transpose(np.argmax(labels, axis=-1))\n",
    "    for s in parts:\n",
    "        if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "            continue\n",
    "        # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "        temp_ = y_val[start_idx:start_idx + int(s)]\n",
    "        temp = y_pred[start_idx:start_idx + int(s)]\n",
    "        if (sum(temp == 0) > sum(temp == 1)):\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "\n",
    "        if (sum(temp_ == 0) > sum(temp_ == 1)):\n",
    "            true.append(0)\n",
    "        else:\n",
    "            true.append(1)\n",
    "\n",
    "        if filenames is not None:\n",
    "            files.append(filenames[start_idx])\n",
    "        start_idx = start_idx + int(s)\n",
    "    \n",
    "    if soft:\n",
    "        pred = cc2parts(y_pred,parts)\n",
    "    return pred,true,files\n",
    "\n",
    "def eerPred(true,pred,verbose=1):\n",
    "    if pred.ndim > 1:\n",
    "            pred = pred[:,-1]\n",
    "    fpr,tpr,thresh = roc_curve(true,pred)\n",
    "    diff = abs(tpr-(1-fpr))\n",
    "    pred = pred > thresh[np.where(diff == min(diff))[0]]\n",
    "    if verbose:\n",
    "        print('Threshold selected as %f'%thresh[np.where(diff == min(diff))[0]])\n",
    "    return pred\n",
    "\n",
    "def calc_metrics(true,pred,files=None,verbose=1,eps=1E-10,thresh=.5):\n",
    "        if thresh=='EER':\n",
    "            TN, FP, FN, TP = confusion_matrix(true, eerPred(true,pred), labels=[0,1]).ravel()\n",
    "        else:\n",
    "            TN, FP, FN, TP = confusion_matrix(true, np.asarray(pred) > thresh, labels=[0,1]).ravel()\n",
    "        sensitivity = TP / (TP + FN + eps)\n",
    "        specificity = TN / (TN + FP + eps)\n",
    "        precision = TP / (TP + FP + eps)\n",
    "        F1 = 2 * (precision * sensitivity) / (precision + sensitivity + eps)\n",
    "        Macc = (sensitivity + specificity) / 2\n",
    "        MCC = (TP*TN-FP*FN)/((TP+FP)*(FN+TN)*(FP+TN)*(TP+FN))**.5\n",
    "        auc = roc_auc_score(true,pred)\n",
    "        logs = dict()\n",
    "        logs['val_sensitivity'] = np.array(sensitivity)\n",
    "        logs['val_specificity'] = np.array(specificity)\n",
    "        logs['val_precision'] = np.array(precision)\n",
    "        logs['val_F1'] = np.array(F1)\n",
    "        logs['val_macc'] = np.array(Macc)\n",
    "        logs['auc'] = np.array(auc)\n",
    "        logs['val_mcc'] = np.array(MCC).astype(np.float64)\n",
    "        if verbose:\n",
    "            print(\"TN:{},FP:{},FN:{},TP:{},Macc:{},F1:{}\".format(TN, FP, FN, TP,Macc,F1))\n",
    "        if files is not None:\n",
    "            true = np.asarray(true)\n",
    "            pred = np.asarray(pred) > .5\n",
    "            files = np.asarray(files)\n",
    "            tpn = true == pred\n",
    "            avg = 0\n",
    "            for dataset in np.unique(files):\n",
    "                mask = files == dataset\n",
    "                avg = avg + np.sum(tpn[mask])/np.sum(mask)/len(np.unique(files))\n",
    "                logs['acc_'+dataset] = np.sum(tpn[mask])/np.sum(mask)\n",
    "            logs['acc_avg'] = avg\n",
    "        df = pd.Series(logs)\n",
    "        return df\n",
    "\n",
    "\n",
    "def log_fusion(logs,data,labels,fusion_weights=None,min_epoch=20,min_metric=.7,\n",
    "               metric='val_macc',model_dir='../models/',verbose=0):        \n",
    "    '''\n",
    "    Returns fused predictions\n",
    "    '''\n",
    "    if not type(logs) == list:\n",
    "        logs = [logs]\n",
    "    \n",
    "    if fusion_weights is None:\n",
    "        fusion_weights = np.ones((len(logs)))\n",
    "    else:\n",
    "        if not len(logs)==len(fusion_weights):\n",
    "            raise ValueError('Fusion weights not consistent with number of models')\n",
    "    pred = np.zeros((data.shape[0],2))\n",
    "    \n",
    "    for log_name,weight in zip(logs,fusion_weights):\n",
    "        model = load_model(log_name,verbose=verbose)\n",
    "        weights = get_weights(log_name,min_epoch=min_epoch,\n",
    "                              min_metric=min_metric,verbose=verbose)\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "        pred += model.predict(data,verbose=verbose)*weight\n",
    "    pred /= sum(fusion_weights)\n",
    "    # pred = np.argmax(pred,axis=-1)\n",
    "    return pred\n",
    "\n",
    "def model_confidence(model,data,labels,verbose=0):\n",
    "    '''\n",
    "    Give confidence score for true class\n",
    "    '''\n",
    "    pred = model.predict(data,verbose=verbose)\n",
    "    \n",
    "    if np.asarray(labels).ndim >1:\n",
    "        labels = np.argmax(labels,axis=-1)\n",
    "    \n",
    "    pred = [pred[idx,each] for idx,each in enumerate(labels)]\n",
    "    \n",
    "    return np.asarray(pred)\n",
    "\n",
    "def plot_coeff(logs,branches=[1,2,3,4],min_epoch=20,min_metric=.7,\n",
    "             metric='val_macc',model_dir='../models/',\n",
    "             figsize=(10,6),verbose=0):\n",
    "    '''\n",
    "    Plot Learnable FIRs for logs\n",
    "    '''\n",
    "    if not type(logs) == list:\n",
    "        logs = [logs]\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, ax = plt.subplots(len(branches), len(logs), sharex='col', sharey='row', figsize=figsize)\n",
    "    \n",
    "    for _idx,log_name in enumerate(logs):\n",
    "        model = load_model(log_name,verbose=verbose)\n",
    "        weights = get_weights(log_name,min_epoch=min_epoch,\n",
    "                              min_metric=min_metric,verbose=verbose)\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "        \n",
    "        FIR_coeff = []\n",
    "        layer_name = []\n",
    "        layer_type = []\n",
    "        \n",
    "        ## Get filter coefficients\n",
    "        for branch in branches:\n",
    "            if not 'gammatone' in model.layers[branch].name:\n",
    "                FIR_coeff.append(np.asarray(model.layers[branch].get_weights())[0,:,0,0])\n",
    "                layer_name.append(model.layers[branch].name)\n",
    "            else: # for gammatone\n",
    "                FIR_coeff.append(K.get_session().run(model.layers[branch].impulse_gammatone()))\n",
    "                layer_name.append(model.layers[branch].name)\n",
    "            try:\n",
    "                layer_type.append(model.layers[branch].FIR_type)\n",
    "            except: # if not linear phase\n",
    "                layer_type.append(0)\n",
    "        \n",
    "        for idx,coeff in enumerate(FIR_coeff):\n",
    "            \n",
    "            ## Flip-concat coefficients for Linearphase\n",
    "            if 'linearphase' in layer_name[idx]:\n",
    "                if layer_type[idx] == 1:\n",
    "                    FIR_coeff[idx] = np.concatenate([np.flip(FIR_coeff[idx][1:],axis=0),FIR_coeff[idx]])  \n",
    "                elif layer_type[idx] == 2:\n",
    "                    FIR_coeff[idx] = np.concatenate([np.flip(FIR_coeff[idx],axis=0),FIR_coeff[idx]])\n",
    "                elif layer_type[idx] == 3:\n",
    "                    FIR_coeff[idx] = np.concatenate([-1*np.flip(FIR_coeff[idx][1:],axis=0),FIR_coeff[idx]])  \n",
    "                else:\n",
    "                    FIR_coeff[idx] = np.concatenate([-1*np.flip(FIR_coeff[idx],axis=0),FIR_coeff[idx]])\n",
    "                    \n",
    "            \n",
    "            ax[idx,_idx].plot((FIR_coeff[idx]-np.mean(FIR_coeff[idx]))/np.std(FIR_coeff[idx]))\n",
    "    \n",
    "    plt.tight_layout()     \n",
    "    plt.show()\n",
    "    return ax\n",
    "    \n",
    "def plot_freq(logs,branches=[1,2,3,4],phase=False,min_epoch=20,min_metric=.7,\n",
    "             metric='val_macc',model_dir='../models/',\n",
    "             figsize=(10,6),verbose=0):\n",
    "    '''\n",
    "    Plot Learnable FIRs for logs\n",
    "    '''\n",
    "    if not type(logs) == list:\n",
    "        logs = [logs]\n",
    "    sns.set_style('whitegrid')\n",
    "    fig, ax = plt.subplots(len(branches), len(logs), sharex='col', sharey='row', figsize=figsize)\n",
    "    \n",
    "    for _idx,log_name in enumerate(logs):\n",
    "        model = load_model(log_name,verbose=verbose)\n",
    "        weights = get_weights(log_name,min_epoch=min_epoch,\n",
    "                              min_metric=min_metric,verbose=verbose)\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "        \n",
    "        FIR_coeff = []\n",
    "        layer_name = []\n",
    "        layer_type = []\n",
    "        \n",
    "        ## Get filter coefficients\n",
    "        for branch in branches:\n",
    "            if not 'gammatone' in model.layers[branch].name:\n",
    "                FIR_coeff.append(np.asarray(model.layers[branch].get_weights())[0,:,0,0])\n",
    "                layer_name.append(model.layers[branch].name)\n",
    "            else: # for gammatone\n",
    "                FIR_coeff.append(K.get_session().run(model.layers[branch].impulse_gammatone()))\n",
    "                layer_name.append(model.layers[branch].name)\n",
    "            try:\n",
    "                layer_type.append(model.layers[branch].FIR_type)\n",
    "            except: # if not linear phase\n",
    "                layer_type.append(0)\n",
    "        \n",
    "        for idx,coeff in enumerate(FIR_coeff):\n",
    "            \n",
    "            ## Flip-concat coefficients for Linearphase\n",
    "            if 'linearphase' in layer_name[idx]:\n",
    "                if layer_type[idx] == 1:\n",
    "                    FIR_coeff[idx] = np.concatenate([np.flip(FIR_coeff[idx][1:],axis=0),FIR_coeff[idx]])  \n",
    "                elif layer_type[idx] == 2:\n",
    "                    FIR_coeff[idx] = np.concatenate([np.flip(FIR_coeff[idx],axis=0),FIR_coeff[idx]])\n",
    "                elif layer_type[idx] == 3:\n",
    "                    FIR_coeff[idx] = np.concatenate([-1*np.flip(FIR_coeff[idx][1:],axis=0),FIR_coeff[idx]])  \n",
    "                else:\n",
    "                    FIR_coeff[idx] = np.concatenate([-1*np.flip(FIR_coeff[idx],axis=0),FIR_coeff[idx]])\n",
    "            \n",
    "            w,freq_res=signal.freqz(FIR_coeff[idx])\n",
    "            ax[idx,_idx].plot(w/np.pi*500,10*np.log10(abs(freq_res)/max(abs(freq_res))))\n",
    "            if phase:\n",
    "                angles = np.unwrap(np.angle(freq_res))\n",
    "                ax2 = ax[idx,_idx].twinx()\n",
    "                ax2.plot(w/np.pi*500, angles, 'g')\n",
    "\n",
    "    plt.tight_layout()     \n",
    "#     plt.show()\n",
    "    return ax\n",
    "\n",
    "def plot_metric(logs,metric='val_loss',smoothing=0.1,lognames=None,xlim=None,ylim=None,\n",
    "                figsize=(10,6),legendLoc=0,colors=None,ax=None):\n",
    "    '''\n",
    "    Plot specified metric for logs\n",
    "    smooth: smoothing factor for each plot \n",
    "    ''' \n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(figsize=figsize)\n",
    "    for idx,log in enumerate(logs):\n",
    "        log_dir='../logs'\n",
    "        if not os.path.isdir(os.path.join(log_dir,log)):\n",
    "            log_dir = '/media/mhealthra2/Data/heart_sound/logs'          \n",
    "        training_csv = os.path.join(log_dir,log,\"training.csv\")\n",
    "        df = pd.read_csv(training_csv)\n",
    "        data = np.asarray(df[metric].values)\n",
    "            \n",
    "        if colors is not None:\n",
    "            ax.plot(smooth(data,smoothing),color=colors[idx])\n",
    "        else:\n",
    "            ax.plot(smooth(data,smoothing))\n",
    "\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "    if lognames is not None:\n",
    "        ax.legend(lognames,loc=legendLoc)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(metric)\n",
    "    \n",
    "    return ax\n",
    "    \n",
    "def plot_log_metrics(log,metrics=['acc_a','acc_e'],labels=None,smoothing=0.1,\n",
    "                     xlim=None,ylim=None,figsize=(10,6),legendLoc=0,colors=None,ax=None):\n",
    "    '''\n",
    "    Plot multiple metrics of the same log\n",
    "    '''\n",
    "    log_dir='../logs'\n",
    "    if not os.path.isdir(os.path.join(log_dir,log)):\n",
    "        log_dir = '/media/mhealthra2/Data/heart_sound/logs'          \n",
    "    training_csv = os.path.join(log_dir,log,\"training.csv\")\n",
    "    df = pd.read_csv(training_csv)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig,ax = plt.subplots(figsize=figsize)\n",
    "    for idx,metric in enumerate(metrics):\n",
    "        data = np.asarray(df[metric].values)\n",
    "        if colors is not None:\n",
    "            ax.plot(smooth(data,smoothing),color=colors[idx])\n",
    "        else:\n",
    "            ax.plot(smooth(data,smoothing))\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "#     if lognames is not None:\n",
    "#         ax.legend(metrics,loc=legendLoc)\n",
    "    ax.set_xlabel('Epochs')\n",
    "    return ax\n",
    "\n",
    "def idx_parts2cc(partidx,parts):\n",
    "    \n",
    "    if type(partidx) == int:\n",
    "        partidx = [partidx]\n",
    "        \n",
    "    idx = []\n",
    "    for each in partidx:\n",
    "        start_idx = int(sum(parts[:each]))\n",
    "        end_idx = int(start_idx + parts[each])\n",
    "        idx = idx+range(start_idx,end_idx)\n",
    "    return idx\n",
    "\n",
    "def grad_cam(model,layer_name,data,label,scale=True,verbose=0):\n",
    "    \n",
    "    if data.ndim < 3:\n",
    "        data = np.expand_dims(data,axis=0)\n",
    "    output = model.output[:,1-int(label)]\n",
    "    last_conv_layer = model.get_layer(layer_name) ##### have to change the name here\n",
    "    grads = K.gradients(output, last_conv_layer.output)[0]\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1)) ### no idea what to do here\n",
    "    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "\n",
    "    pooled_grads_value, conv_layer_output_value = iterate([data])\n",
    "    for i in range(pooled_grads_value.shape[0]):\n",
    "        if verbose:\n",
    "            print(\"Iteration %d\" % i)\n",
    "        conv_layer_output_value[ :, i] *= pooled_grads_value[i]\n",
    "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    if scale:\n",
    "        x = np.linspace(0, data.shape[1], num=len(heatmap))\n",
    "        y = heatmap\n",
    "        f1 = interp1d(x, y, kind='cubic')\n",
    "        xnew = np.linspace(0, data.shape[1], num=data.shape[1])\n",
    "        ynew = f1(xnew)\n",
    "        return ynew\n",
    "    else:\n",
    "        return heatmap\n",
    "\n",
    "def cc2rec(data):\n",
    "    rec = []\n",
    "    for cc in data:\n",
    "        idx = np.where(cc!=0)[0]\n",
    "        cc = cc[:idx[-1],0]\n",
    "        rec.append(cc)\n",
    "    return np.asarray(np.hstack(rec))\n",
    "\n",
    "def cc2rec_labels(data,labels):\n",
    "    gt = []\n",
    "    for i,cc in enumerate(data):\n",
    "        idx = np.where(cc!=0)[0]\n",
    "        cctr = np.ones(idx[-1])*labels[i]\n",
    "        gt.append(cctr)    \n",
    "    return np.asarray(np.hstack(gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_coeff([\n",
    "            \"potes_fold0_noFIR 2019-03-02 13_01_33.636778\",\n",
    "            \"fold0_noFIR 2019-03-07 14_44_47.022240\"\n",
    "#             \"fold0_noFIR 2019-02-24 18_02_57.053839\",\n",
    "            \n",
    "#           \"fold0_noFIR 2019-02-27 19_52_21.543329\",\n",
    "#           \"fold2_noFIR 2019-01-17 04_16_51.868927\", # random\n",
    "#           \"fold1_noFIR 2019-01-13 15_04_39.094472\", \n",
    "#           \"fold1_noFIR 2019-02-16 12_28_19.127331\", # densenet\n",
    "#             \"fold0_noFIR 2019-03-06 14_21_29.823568\", # bi-conv stage1\n",
    "#            \"fold0_noFIR 2019-03-06 21_42_10.719836\", # bi-conv stage2\n",
    "#           \"fold0_noFIR 2019-03-09 01_34_03.547265\", #gamma stage 1\n",
    "#             \"fold0_noFIR 2019-03-09 07_12_26.773316\", #gamma stage 2\n",
    "#             \"fold0_noFIR 2019-03-08 03_28_46.740442\", #type3\n",
    "#             \"potes_fold0_noFIR 2019-03-16 18_44_45.597226\"\n",
    "         ],min_epoch=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foldname = 'fold1+compare'\n",
    "fold_dir = '../data/feature/folds/'\n",
    "\n",
    "# x_train, y_train, train_files,train_parts, q_train, \\\n",
    "#     x_val, y_val,val_files,val_parts, q_val = load_data(foldname,fold_dir,quality=True)\n",
    "    \n",
    "# test_parts = train_parts[0][np.asarray(train_files) =='x']\n",
    "# test_parts = np.concatenate([test_parts,val_parts[np.asarray(val_files)=='x']],axis=0)\n",
    "# train_files = parts2cc(train_files,train_parts[0])\n",
    "# val_files = parts2cc(val_files,val_parts)\n",
    "# x_test = x_train[train_files == 'x']\n",
    "# x_test = np.concatenate([x_test,x_val[val_files=='x']])\n",
    "# y_test = y_train[train_files == 'x']\n",
    "# y_test = np.concatenate([y_test,y_val[val_files=='x']])\n",
    "# test_files = np.concatenate([train_files[train_files == 'x'],\n",
    "#                             val_files[val_files == 'x']])\n",
    "# q_test = np.concatenate([q_train[train_files == 'x'],\n",
    "#                             q_val[val_files == 'x']])\n",
    "# del x_train, y_train, train_files,train_parts, q_train, \\\n",
    "#     x_val, y_val,val_files,val_parts, q_val\n",
    "    \n",
    "    \n",
    "foldname = 'fold0_noFIR'\n",
    "x_train, y_train, train_files,train_parts, q_train, \\\n",
    "    x_val, y_val,val_files,val_parts, q_val = load_data(foldname,fold_dir,quality=True) # also return recording quality\n",
    "\n",
    "train_parts = train_parts[np.nonzero(train_parts)] ## Some have zero cardiac cycle\n",
    "val_parts = val_parts[np.nonzero(val_parts)]\n",
    "\n",
    "x_test = x_val\n",
    "y_test = y_val\n",
    "test_parts = val_parts\n",
    "test_files = val_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test2Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# frac=.3\n",
    "# print('Before Test partition')\n",
    "# print(x_train.shape, x_test.shape, y_train.shape, y_test.shape, train_parts.shape, test_parts.shape)\n",
    "\n",
    "# test_files = np.repeat('g',len(test_files),axis=0)\n",
    "# random_seed = 1\n",
    "# np.random.seed(random_seed)\n",
    "# part_idx = np.random.permutation(range(len(test_parts)))\n",
    "# part_idx = part_idx[:int(len(test_parts) * frac)]\n",
    "# cc_idx = idx_parts2cc(part_idx, test_parts)\n",
    "\n",
    "# # train_parts = np.concatenate([train_parts, test_parts[part_idx]], axis=0)\n",
    "# test_parts = np.delete(test_parts, part_idx, axis=0)\n",
    "# # val_parts = np.concatenate([val_parts, test_parts], axis=0)\n",
    "\n",
    "# # train_files = np.concatenate([train_files, test_files[cc_idx]], axis=0)\n",
    "# test_files = np.delete(test_files, cc_idx, axis=0)\n",
    "# # val_files = np.concatenate([val_files, test_files], axis=0)\n",
    "\n",
    "# # x_train = np.concatenate([x_train, x_test[cc_idx]], axis=0)\n",
    "# x_test = np.delete(x_test, cc_idx, axis=0)\n",
    "# # x_val = np.concatenate([x_val, x_test], axis=0)\n",
    "\n",
    "# # y_train = np.concatenate([y_train, y_test[cc_idx]], axis=0)\n",
    "# y_test = np.delete(y_test, cc_idx, axis=0)\n",
    "# # y_val = np.concatenate([y_val, y_test], axis=0)\n",
    "\n",
    "# print('After Test partition with fraction', frac)\n",
    "# print(x_train.shape, x_test.shape, y_train.shape, y_test.shape, train_parts.shape, test_parts.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Model\n",
    "#### To to the McNemer test run Model.Predict with Type2 log_name\n",
    "#### The come back and uncomment the potes log_name then run until Model.Predict again.\n",
    "#### Finally run the McNembers Test block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Heartnet\n",
    "log_name = \"fold0_noFIR 2019-03-07 14_44_47.022240\" # Type2 macc 80 epoch\n",
    "\n",
    "## Potes \n",
    "# log_name = \"potes_fold0_noFIR 2019-03-02 13_01_33.636778\"\n",
    "\n",
    "\n",
    "# log_name = \"fold0_noFIR 2019-02-24 18_02_57.053839\"\n",
    "# log_name = \"fold0_noFIR 2019-03-09 01_34_03.547265\"\n",
    "# log_name = \"fold0_noFIR 2019-02-24 18_02_57.053839\" # Type1 macc\n",
    "# log_name = \"potes_fold0_noFIR 2019-03-02 13_01_33.636778\" # potes\n",
    "# log_name = \"fold0_noFIR 2019-03-09 01_34_03.547265\" #gamma stage 1\n",
    "\n",
    "# log_name = \"fold0_noFIR 2019-03-08 03_28_46.740442\" # Type3 sensitivity/spec for balanced\n",
    "# log_name = \"fold0_noFIR 2019-03-08 14_50_52.332924\" # type4 val_acc\n",
    "# log_name = \"fold0_noFIR 2019-03-06 21_42_10.719836\" #zero stage2\n",
    "# log_name = \"potes_fold0_noFIR 2019-03-16 18_44_45.597226\" #potes non balanced\n",
    "\n",
    "### Trained with both train and test\n",
    "\n",
    "# log_name = \"fold0_noFIR 2019-03-24 18_55_14.833080\" #frac=.1\n",
    "# log_name = \"fold0_noFIR 2019-03-24 23_14_47.400720\" #frac=.2\n",
    "# log_name = 'fold0_noFIR 2019-03-25 03_34_29.171850' #frac=.3\n",
    "\n",
    "### Fine tuned with test\n",
    "\n",
    "# log_name = \"fold0_noFIR 2019-04-16 14_48_05.398094\"\n",
    "# log_name = \"fold0_noFIR 2019-04-20 17_44_16.413759\"\n",
    "\n",
    "model = load_model(log_name,verbose=1)\n",
    "weights = get_weights(log_name,min_epoch=80,min_metric=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'val_macc'\n",
    "model_dir = '../models/'\n",
    "\n",
    "checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "model.load_weights(checkpoint_name)\n",
    "print(\"Checkpoint loaded:\\n %s\" % checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_num = 1\n",
    "# model_dir = '/media/mhealthra2/Data/heart_sound/models/'\n",
    "\n",
    "# checkpoint_name = os.path.join(model_dir+log_name,weights['epoch'][epoch_num])\n",
    "# model.load_weights(checkpoint_name)\n",
    "# print(\"Checkpoint loaded:\\n %s\" % checkpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Calculating metrics for Training set')\n",
    "# pred,true,files = predict_parts(model,x_train,y_train,train_parts,train_files)\n",
    "# res = calc_metrics(true,pred,files)\n",
    "# print(res)\n",
    "\n",
    "\n",
    "# print('Calculating metrics for all of Validation')\n",
    "# pred,true,files = predict_parts(model,x_val,y_val,val_parts,val_files,soft=True)\n",
    "# res = calc_metrics(true,pred,files)\n",
    "# print(res)\n",
    "\n",
    "print('\\n\\nCalculating metrics for test')\n",
    "pred,true,files = predict_parts(model,x_test,y_test,test_parts,test_files,soft=True)\n",
    "res = calc_metrics(true,pred,files)\n",
    "print(res)\n",
    "\n",
    "###########################   added by rakib   #########################\n",
    "## Keeping the results of the validation set\n",
    "## Run with a potes model, and with a proposed model\n",
    "pred_potes = None\n",
    "pred_proposed = None\n",
    "if(\"potes\" in log_name):\n",
    "    pred_potes = pred\n",
    "else:\n",
    "    pred_proposed = pred\n",
    "    \n",
    "##################################################################\n",
    "    \n",
    "# print('\\n\\nCalculating metrics for good quality only')\n",
    "# pred,true,files = predict_parts(model,\n",
    "#                                 x_val[q_val>0],y_val[q_val>0],\n",
    "#                                 val_parts[cc2parts(q_val,val_parts)>0],\n",
    "#                                 np.asarray(val_files)[q_val>0])\n",
    "# res = calc_metrics(true,pred,files)\n",
    "# print(res)\n",
    "\n",
    "##################   Not running the test data  ###############\n",
    "\n",
    "# print('\\n\\nCalculating metrics for test')\n",
    "# pred,true,files = predict_parts(model,x_test,y_test,test_parts,test_files,soft=True)\n",
    "# res = calc_metrics(true,pred,files)\n",
    "# print(res)\n",
    "# pred_fir4 = pred\n",
    "# pred_proposed=pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRoc(y_true, y_pred,ax=None,label='',control=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    By providing an plt.subplot's \"ax\" instance you can plot multiple roc's on the same plot by \n",
    "    calling this function multiple times with the same \"ax\"\n",
    "    If none then it will generate multiple plots\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    lr_probs = y_pred\n",
    "    testy = y_true\n",
    "    ns_probs = [0 for _ in range(len(testy))]\n",
    "    # keep probabilities for the positive outcome only\n",
    "    #lr_probs = lr_probs[:, 1]\n",
    "    # calculate scores\n",
    "    ns_auc = roc_auc_score(testy, ns_probs)\n",
    "    lr_auc = roc_auc_score(testy, lr_probs)\n",
    "    # summarize scores\n",
    "    print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "    print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)\n",
    "    # plot the roc curve for the model\n",
    "    if(ax is not None):\n",
    "        if control:\n",
    "            ax.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "        ax.plot(lr_fpr, lr_tpr,  label=label)\n",
    "        # axis labels\n",
    "        ax.set_xlabel('False Positive Rate',fontdict={'size':16})\n",
    "        ax.set_ylabel('True Positive Rate',fontdict={'size':16})\n",
    "        # show the legend\n",
    "        ax.legend( prop={'size':15})\n",
    "    else:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "        plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic ')\n",
    "        # axis labels\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        # show the legend\n",
    "        plt.legend()\n",
    "        # show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "if(pred_proposed is not None):\n",
    "    plotRoc(true,pred_proposed,ax=ax,label='Type 2 tConv: AUC 0.83')\n",
    "if(pred_potes is not None):\n",
    "    plotRoc(true,pred_potes,ax=ax,label='Potes-CNN: AUC 0.499',control=False)\n",
    "# plotRoc(true,pred_fir4,ax=ax,label='TypeIV tConv-CNN: AUC 0.864',control=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('roc.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McNemer Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "def McnemerStats(true,pred1,pred2,threshold = 0.05, lab1='model 1', lab2='model 2'):\n",
    "    \"\"\"\n",
    "    true: true labels of the data. \n",
    "    pred1: predicted values from model 1 ## generated from block 8\n",
    "    pred2: predicted values from model 2 ## generated from block 8\n",
    "    \"\"\"\n",
    "    pred1 = 1*(np.asarray(pred1) > 0.5)\n",
    "    pred2 = 1*(np.asarray(pred2) > .5)\n",
    "    true = np.asarray(true)\n",
    "    print(pred1.shape, pred2.shape, true.shape)\n",
    "    mc_table = mcnemar_table(y_target=true, \n",
    "                   y_model1=pred1, \n",
    "                   y_model2=pred2)\n",
    "    if(np.min(mc_table)<25):\n",
    "        result = mcnemar(mc_table, exact=True)\n",
    "    else:\n",
    "        result = mcnemar(mc_table, exact=False, correction=True)\n",
    "    print('statistic=%.10f, p-value=%.10f' % (result.statistic, result.pvalue))\n",
    "    # interpret the p-value\n",
    "    alpha = threshold\n",
    "    if result.pvalue > alpha:\n",
    "        print('Same proportions of errors, models make similar error (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different proportions of errors, error rates are different (reject H0)')\n",
    "    return result.statistic,result.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(pred_potes is None):\n",
    "    print(\"Please run the model with the potes Model\")\n",
    "if(pred_proposed is None):\n",
    "    print(\"Please run the model with the hearnet type2 Model\")\n",
    "if(pred_potes is not None and pred_proposed is not None):\n",
    "    McnemerStats(true,pred_proposed,pred_potes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabor vs Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabor = pd.read_csv('gabor_result.csv')\n",
    "val_wav = pd.read_csv('val_file_names.txt',header=None)\n",
    "val_wav = [x[0] for x in val_wav.values]\n",
    "gtrue =[(gabor.loc[gabor['filenames']=='train_'+x[:-4],'true'].iloc[0]) for x in val_wav]\n",
    "gpred =[(gabor.loc[gabor['filenames']=='train_'+x[:-4],'pred'].iloc[0]) for x in val_wav]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "McnemerStats(true,pred_proposed,gpred,lab1='heartnet',lab2='gabor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plotRoc(true,pred_proposed,ax=ax,label='GammaTone tConv: AUC 0.798')\n",
    "plotRoc(true,gpred,ax=ax,label=\"Gabor's algorithm: AUC 0.449\",control=False)\n",
    "# plotRoc(true,pred_fir4,ax=ax,label='TypeIV tConv-CNN: AUC 0.864',control=False)\n",
    "fig.savefig('gabor_vs_gammtonetconv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.auc(true,gpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EER\n",
    "# pred = model.predict(x_train)\n",
    "# pred = cc2parts(pred,train_parts)[:,1]\n",
    "# true = cc2parts(y_train,train_parts)[:,1]\n",
    "# files = cc2parts(train_files,train_parts)\n",
    "# res = calc_metrics(true,pred,files,thresh='EER')\n",
    "# print(res)\n",
    "\n",
    "\n",
    "# pred = model.predict(x_val)\n",
    "# pred = cc2parts(pred,val_parts)[:,1]\n",
    "# true = cc2parts(y_val,val_parts)[:,1]\n",
    "# files = cc2parts(val_files,val_parts)\n",
    "# res = calc_metrics(true,pred,files,thresh='EER')\n",
    "# print(res)\n",
    "\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "pred = cc2parts(pred,test_parts)[:,1]\n",
    "true = cc2parts(y_test,test_parts)[:,1]\n",
    "files = cc2parts(test_files,test_parts)\n",
    "res = calc_metrics(true,pred,files,thresh='EER')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calc_metrics(true,np.random.rand(len(true)),thresh='EER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "# from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_val)\n",
    "preds = cc2parts(preds[:,1],val_parts)\n",
    "true = cc2parts(y_val[:,1],val_parts)\n",
    "fpr,tpr,thresh = roc_curve(true,preds)\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr)\n",
    "diff = abs(tpr-(1-fpr))\n",
    "preds = preds > thresh[np.where(diff == min(diff))[0]]\n",
    "print(thresh[np.where(diff == min(diff))[0]])\n",
    "\n",
    "calc_metrics(true,preds,cc2parts(val_files,val_parts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test)\n",
    "preds = cc2parts(preds[:,1],test_parts)\n",
    "true = cc2parts(y_test[:,1],test_parts)\n",
    "fpr,tpr,thresh = roc_curve(true,preds)\n",
    "plt.figure()\n",
    "plt.plot(fpr,tpr)\n",
    "diff = abs(tpr-(1-fpr))\n",
    "preds = preds > thresh[np.where(diff == min(diff))[0]]\n",
    "print(thresh[np.where(diff == min(diff))[0]])\n",
    "\n",
    "calc_metrics(true,preds,cc2parts(test_files,test_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=1,return_indices=True)\n",
    "_,y,partidx = rus.fit_resample(np.expand_dims(range(len(test_parts)),axis=-1),cc2parts(y_test[:,1],test_parts))\n",
    "ccidx= idx_parts2cc(partidx,test_parts)\n",
    "_parts = test_parts[partidx]\n",
    "x = x_test[ccidx]\n",
    "y = y_test[ccidx]\n",
    "_files = test_files[ccidx]\n",
    "\n",
    "pred,true,files = predict_parts(model,x,y,_parts,_files,soft=True)\n",
    "res = calc_metrics(true,pred,files,thresh='EER')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Fusion predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fusion Predict Val')\n",
    "model_dir = '../models/'\n",
    "# fusion_weights = [.8,1.2,.8,1.2]\n",
    "fusion_weights = [1,1,.4,1]\n",
    "\n",
    "pred = np.zeros((x_val.shape[0],2))\n",
    "for metric,weight in zip(weights.keys(),fusion_weights):\n",
    "    checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "    model.load_weights(checkpoint_name)\n",
    "    pred += model.predict(x_val,verbose=1)*weight\n",
    "pred /= sum(fusion_weights)\n",
    "# pred = np.argmax(pred,axis=-1)\n",
    "pred = pred[:,1]\n",
    "res = calc_metrics(cc2parts(np.argmax(y_val,axis=-1),val_parts),np.round(cc2parts(pred,val_parts)))\n",
    "print(res)\n",
    "\n",
    "print('\\n\\nFusion Predict Test')\n",
    "pred = np.zeros((x_test.shape[0],2))\n",
    "for metric,weight in zip(weights.keys(),fusion_weights):\n",
    "    checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "    model.load_weights(checkpoint_name)\n",
    "    \n",
    "    pred += model.predict(x_test,verbose=1)*weight\n",
    "pred /= sum(fusion_weights)\n",
    "# pred = np.argmax(pred,axis=-1)\n",
    "pred = pred[:,1]\n",
    "res = calc_metrics(cc2parts(np.argmax(y_test,axis=-1),test_parts),np.round(cc2parts(pred,test_parts)))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fold model fusion predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs=[\n",
    "(0,\"fold0_noFIR 2019-02-24 18:02:57.053839\",'val_macc',100,.7), # Type1 macc\n",
    "(1,\"fold0_noFIR 2019-03-09 01:34:03.547265\",'val_macc',100,.7), #gamma stage 1\n",
    "(0,\"fold0_noFIR 2019-03-07 14:44:47.022240\",'val_macc',80,.7), # Type2 macc 80 epoch\n",
    "(0,\"fold0_noFIR 2019-03-08 03:28:46.740442\",'val_sensitivity',100,.65), # Type3 sensitivity/spec for balanced\n",
    "(0,\"fold0_noFIR 2019-03-08 14:50:52.332924\",'val_acc',100,.7), # type4 val_acc\n",
    "(0,\"fold0_noFIR 2019-03-06 21:42:10.719836\",'val_macc',100,.7), #zero stage2\n",
    "]\n",
    "pred_fusion=0\n",
    "for weight,log,metric,epoch,min_metric in logs:\n",
    "    if not weight:\n",
    "        continue\n",
    "    model=load_model(log_name=log)\n",
    "    model_weights = get_weights(log_name=log,\n",
    "                                min_epoch=epoch,\n",
    "                                min_metric=min_metric)\n",
    "    model_dir = '../models/'\n",
    "    checkpoint_name = os.path.join(model_dir+log,model_weights[metric])\n",
    "    model.load_weights(checkpoint_name)\n",
    "    pred = model.predict(x_test)\n",
    "    pred = cc2parts(pred,test_parts)\n",
    "    pred_fusion += weight*pred\n",
    "\n",
    "pred_fusion /= sum([each[0] for each in logs])\n",
    "# pred_fusion = cc2parts(pred_fusion,test_parts)\n",
    "print(calc_metrics(true=cc2parts(y_test,test_parts)[:,1],pred=pred_fusion[:,1],verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = [\n",
    "    \"fold0_noFIR 2019-02-24 18:02:57.053839\", #Type1\n",
    "#     \"fold1_noFIR 2019-02-23 17:59:17.240365\"\n",
    " \n",
    "           ]\n",
    "pred = log_fusion(logs,x_test,y_test,min_metric=.7,\n",
    "                  metric='val_specificity',verbose=0)\n",
    "pred = pred[:,1]\n",
    "res = calc_metrics(cc2parts(np.argmax(y_test,axis=-1),test_parts),\n",
    "                   np.round(cc2parts(pred,test_parts)))\n",
    "print(res.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logs=[\n",
    "\"potes_fold0_noFIR 2019-03-02 13:01:33.636778\", # potes\n",
    "\"fold0_noFIR 2019-02-24 18:02:57.053839\", # Type1 macc\n",
    "\"fold0_noFIR 2019-03-07 14:44:47.022240\", # Type2 macc 80 epoch\n",
    "\"fold0_noFIR 2019-03-08 03:28:46.740442\", # Type3 sensitivity\n",
    "\"fold0_noFIR 2019-03-08 14:50:52.332924\", # type4 val_acc\n",
    "\"fold0_noFIR 2019-03-09 01:34:03.547265\", # gamma stage 1\n",
    "\"fold0_noFIR 2019-03-06 21:42:10.719836\", # zero stage2\n",
    "]\n",
    "lognames=[\n",
    "\"Static FIR\",\n",
    "\"Type I tConv\",\n",
    "\"Type II tConv\",\n",
    "\"Type III tConv\",\n",
    "\"Type IV tConv\",\n",
    "\"Gammatone tConv\",\n",
    "\"Zero Phase tConv\",\n",
    "]\n",
    "branchnames=[\n",
    "'Branch 1',\n",
    "'Branch 2',\n",
    "'Branch 3',\n",
    "'Branch 4',\n",
    "]\n",
    "ax = plot_freq(logs=logs,min_epoch=100,metric='val_macc',min_metric=.6,figsize=(17,7),phase=True)\n",
    "# ax[3,0].set_ylim([-6,6])\n",
    "# ax[3,2].set_xlim([0,59])\n",
    "# ax[3,4].set_xlim([0,59])\n",
    "# for axes,branch in zip(ax[:,0],branchnames):\n",
    "#     axes.set_ylabel('%s Gain' % branch)\n",
    "for axes,log in zip(ax[3,:],lognames):\n",
    "    axes.set_xlabel('%s Weights' % log)\n",
    "# plt.subplots_adjust(left=0.035,bottom=0.065)\n",
    "# # plt.savefig('coeffs.eps')\n",
    "# # plt.savefig('coeffs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for axes,log in zip(ax[3,:],lognames):\n",
    "#     axes.set_xlabel('%s Weights' % log)\n",
    "# plt.subplots_adjust(left=0.035,bottom=0.065)\n",
    "plt.savefig('coeffsFreq.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs=[\n",
    "\"potes_fold0_noFIR 2019-03-16 18:44:45.597226\", # potes non balanced\n",
    "\"potes_fold0_noFIR 2019-03-02 13:01:33.636778\", # potes\n",
    "\"fold0_noFIR 2019-02-27 19:52:21.543329\", # Type1 macc\n",
    "\"fold0_noFIR 2019-03-07 14:44:47.022240\", # Type2 macc 80 epoch\n",
    "\"fold0_noFIR 2019-03-08 03:28:46.740442\", # Type3 sensitivity\n",
    "\"fold0_noFIR 2019-03-08 14:50:52.332924\", # type4 val_acc\n",
    "\"fold0_noFIR 2019-03-09 01:34:03.547265\", # gamma stage 1\n",
    "\"fold0_noFIR 2019-03-06 14:21:29.823568\", # zero stage2\n",
    "]\n",
    "lognames=[\n",
    "\"Potes-CNN\",\n",
    "\"Potes-CNN DBT\",\n",
    "\"Type I tConv\",\n",
    "\"Type II tConv\",\n",
    "\"Type III tConv\",\n",
    "\"Type IV tConv\",\n",
    "\"Gammatone tConv\",\n",
    "\"Zero Phase tConv\",\n",
    "]\n",
    "colors = [\n",
    "'#434B77',\n",
    "'#669966',\n",
    "'#c10061',\n",
    "'#ff51a5',\n",
    "'k',\n",
    "'#ffbe4f',\n",
    "#'#008080',\n",
    "'#DBBBBB',\n",
    "'#008080',\n",
    "         ]\n",
    "plot_metric(logs,lognames=lognames,smoothing=0.5,metric='val_loss',colors=colors,figsize=(10,8.5))\n",
    "plt.ylabel('Validation Loss per Cardiac Cycle')\n",
    "plt.ylim([0.44,0.65])\n",
    "# plt.savefig('validationLoss.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[\n",
    "#     'acc_a',\n",
    "    'acc_b',\n",
    "#     'acc_c',\n",
    "#     'acc_d',\n",
    "    'acc_e'\n",
    "]\n",
    "labels=[\n",
    "    'subset-a',\n",
    "#     'subset-b',\n",
    "#     'subset-c',\n",
    "#     'subset-d',\n",
    "    'subset-e'\n",
    "]\n",
    "ax = plot_metric([logs[0],logs[2]],metrics,smoothing=0.7,legendLoc=0,ylim=[.4,1.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_ylabel('Subset-wise Validation Accuracy')\n",
    "ax.legend(['Subset-a w/o DBT','Subset-e w/o DBT','Subset-a w/ DBT','Subset-e w/ DBT'],loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Activations and TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recBins = [117,385,7,27,1867,80,116,292,104,24,28,151,34,566];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_labels = np.asarray([ord(each)- 97 for each in train_files+val_files+list(test_files)])\n",
    "meta_labels[meta_labels == 23] = 6\n",
    "y = np.argmax(np.concatenate([y_train,y_val,y_test]),axis=-1)\n",
    "\n",
    "for idx,each in enumerate(np.unique(meta_labels)):\n",
    "        indices = np.where(np.logical_and(y==1,meta_labels == each))\n",
    "        meta_labels[indices] = 7 +idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activations = np.array(get_activations(model,np.concatenate([x_train,x_val,x_test],axis=0),\n",
    "                                       batch_size=64,layer_name='flatten_1'))\n",
    "if activations.ndim > 2:\n",
    "    activations = np.reshape(activations,(len(activations),-1))\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_labels=meta_labels[0:len(activations)]\n",
    "quality_labels=np.concatenate([q_train,q_val,q_test],axis=0)[0:len(activations)]\n",
    "\n",
    "idx = []\n",
    "for subset,each in zip(np.unique(meta_labels),recBins):\n",
    "    np.random.seed(1)\n",
    "    idx = idx+list(np.random.choice(np.where([meta_labels==subset])[1],size=(each,),replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rus = RandomUnderSampler(random_state=1,return_indices=True)\n",
    "# x,y,idx = rus.fit_resample(activations[quality_labels>0],meta_labels[quality_labels>0])\n",
    "# np.random.seed(1)\n",
    "# idx = np.random.choice(range(len(meta_labels)),size=(3792,),replace=False)\n",
    "x = activations[idx]\n",
    "y = meta_labels[idx]\n",
    "X_embed = scale(x)\n",
    "\n",
    "# X_embedded = PCA(n_components=50).fit_transform(X_embed)\n",
    "\n",
    "X_embedded = TSNE(n_components=2,\n",
    "#                   learning_rate=60,\n",
    "#                   early_exaggeration=1140.,\n",
    "                  perplexity=480, #480-2, 150-3 without exagg and lr\n",
    "                  init='random',\n",
    "                  n_iter=4000,\n",
    "                  verbose=1,\n",
    "                  ).fit_transform(X_embed)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "import matplotlib.font_manager as font_manager\n",
    "font_prop = font_manager.FontProperties(size=14)\n",
    "font_title = font_manager.FontProperties(size=20)\n",
    "\n",
    "colors = ['#434B77',\n",
    "          '#669966',\n",
    "          '#c10061',\n",
    "          '#ff51a5',\n",
    "          'k',\n",
    "          '#ffbe4f',\n",
    "#           '#008080',\n",
    "          '#DBEEEE',\n",
    "          '#008080',\n",
    "         ]\n",
    "# y_ = y_>6\n",
    "subsets = [\"Eko CORE Bluetooth\",\n",
    "\"Welch Allyn Meditron\",\n",
    "\"3M Littmann E4000\",\n",
    "\"AUDIOSCOPE\",\n",
    "\"Infral Corp. Prototype\",\n",
    "\"MLT201/Piezo\",\n",
    "\"JABES\",\n",
    "\"3M Littmann\"]\n",
    "parser = dict(zip(np.unique(y_),subsets))\n",
    "fig = plt.figure(figsize=(11,8))\n",
    "for stage,color in zip(np.unique(y_),colors):\n",
    "    mask = y_ == stage\n",
    "    plt.scatter(X_embedded[mask,0],X_embedded[mask,1],c=color,label=parser[stage],s=30)\n",
    "plt.legend(markerscale=2,fontsize=14)\n",
    "fig.set_tight_layout(tight=1)\n",
    "plt.xlabel('TSNE Component 1',fontproperties=font_prop)\n",
    "plt.ylabel('TSNE Component 2',fontproperties=font_prop)\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('potesTSNE.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('gammaTSNEbal.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_labels = np.asarray([ord(each)- 97 for each in train_files+val_files+list(test_files)])\n",
    "meta_labels[meta_labels == 23] = 6\n",
    "y = np.argmax(np.concatenate([y_train,y_val,y_test]),axis=-1)\n",
    "\n",
    "for idx,each in enumerate(np.unique(meta_labels)):\n",
    "        indices = np.where(np.logical_and(y==1,meta_labels == each))\n",
    "        meta_labels[indices] = 7 +idx\n",
    "meta_labels=meta_labels[0:len(activations)]\n",
    "quality_labels=np.concatenate([q_train,q_val,q_test],axis=0)[0:len(activations)]\n",
    "\n",
    "idx = []\n",
    "for subset,each in zip(np.unique(meta_labels),recBins):\n",
    "    np.random.seed(1)\n",
    "    idx = idx+list(np.random.choice(np.where([meta_labels==subset])[1],size=(each,),replace=False))\n",
    "    \n",
    "y_= meta_labels[idx]\n",
    "y_[y_==11] = 14\n",
    "y_[y_>6] = y_[y_>6] - 7 # 0-7 steth labels\n",
    "y_ = y_+1\n",
    "y_[y_==7] = 0\n",
    "y_[y_==8] = 7\n",
    "print(np.unique(y_),np.bincount(y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recording Level TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.asarray(train_files+val_files+list(test_files))\n",
    "parts = np.asarray(list(train_parts)+list(val_parts)+list(test_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([x_train,x_val,x_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = np.array(get_activations(model,data[:-13],\n",
    "                                       batch_size=64,layer_name='flatten_1'))\n",
    "rem = np.array(get_activations(model,data[-13:],\n",
    "                                       batch_size=1,layer_name='flatten_1'))\n",
    "activations = np.concatenate([activations,rem],axis=0)\n",
    "\n",
    "del data, rem\n",
    "\n",
    "if activations.ndim > 2:\n",
    "    activations = np.reshape(activations,(len(activations),-1))\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = cc2parts(files,parts)\n",
    "activations = cc2parts(activations,parts)\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_labels = np.asarray([ord(each)- 97 for each in files])\n",
    "meta_labels[meta_labels == 23] = 6\n",
    "y = cc2parts(np.argmax(np.concatenate([y_train,y_val,y_test]),axis=-1),parts)\n",
    "\n",
    "for idx,each in enumerate(np.unique(meta_labels)):\n",
    "        indices = np.where(np.logical_and(y==1,meta_labels == each))\n",
    "        meta_labels[indices] = 7 +idx\n",
    "np.unique(meta_labels)\n",
    "\n",
    "y_= meta_labels\n",
    "y_[y_==11] = 14\n",
    "y_[y_>6] = y_[y_>6] - 7 # 0-7 steth labels\n",
    "y_ = y_+1\n",
    "y_[y_==7] = 0\n",
    "y_[y_==8] = 7\n",
    "print(np.unique(y_),np.bincount(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "savemat('typeII.mat',{'X':activations,'y':y_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embed = scale(activations)\n",
    "# X_embedded = PCA(n_components=50).fit_transform(X_embed)\n",
    "X_embedded = TSNE(n_components=2,\n",
    "#                   learning_rate=60,\n",
    "#                   early_exaggeration=1140.,\n",
    "                  perplexity=480, #480-2, 150-3 without exagg and lr\n",
    "                  init='random',\n",
    "                  n_iter=4000,\n",
    "                  verbose=1,\n",
    "                  ).fit_transform(X_embed)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "import matplotlib.font_manager as font_manager\n",
    "font_prop = font_manager.FontProperties(size=14)\n",
    "font_title = font_manager.FontProperties(size=20)\n",
    "\n",
    "colors = ['#434B77',\n",
    "          '#669966',\n",
    "          '#c10061',\n",
    "          '#ff51a5',\n",
    "          'k',\n",
    "          '#ffbe4f',\n",
    "#           '#008080',\n",
    "          '#DBEEEE',\n",
    "          '#008080',\n",
    "         ]\n",
    "# y_ = y_>6\n",
    "subsets = [\"Eko CORE Bluetooth\",\n",
    "\"Welch Allyn Meditron\",\n",
    "\"3M Littmann E4000\",\n",
    "\"AUDIOSCOPE\",\n",
    "\"Infral Corp. Prototype\",\n",
    "\"MLT201/Piezo\",\n",
    "\"JABES\",\n",
    "\"3M Littmann\"]\n",
    "parser = dict(zip(np.unique(y_),subsets))\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "for stage,color in zip(np.unique(y_),colors):\n",
    "    mask = y_ == stage\n",
    "    plt.scatter(X_embedded[mask,0],X_embedded[mask,1],c=color,label=parser[stage])\n",
    "plt.legend(markerscale=2,fontsize=14)\n",
    "fig.set_tight_layout(tight=1)\n",
    "plt.xlabel('TSNE Component 1',fontproperties=font_prop)\n",
    "plt.ylabel('TSNE Component 2',fontproperties=font_prop)\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('rec_gammatoneTSNE.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('rec_typeIITSNE.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "conf = model_confidence(model,x_val,y_val)\n",
    "conf = cc2parts(conf,val_parts)\n",
    "plt.hist(conf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potes = \"/media/mhealthra2/Data/Heart_Sound/Physionet/answers.txt\"\n",
    "pdf = pd.read_csv(potes, header=None)\n",
    "pdf.set_index(0,inplace=True)\n",
    "gt = \"/media/mhealthra2/Data/Heart_Sound/Physionet/2016-07-25_Updated files for Challenge 2016/Online Appendix_training set.csv\"\n",
    "gtdf = pd.read_csv(gt)\n",
    "gtdf.set_index('Challenge record name',inplace=True)\n",
    "pdf = pdf.join(gtdf,how='left')\n",
    "files = pdf.index.str\n",
    "calc_metrics(true=pdf['Class (-1=normal 1=abnormal)']>0,pred=pdf[1]>0,files=pdf.index.str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam_rec(model,layer_name,cc,label,output_class='true',normalize=True,verbose=0):\n",
    "    '''\n",
    "    Generate class activation maps for whole recording\n",
    "    \n",
    "    Inputs:\n",
    "    model: model object\n",
    "    layer_name: layer to take grads of\n",
    "    cc: segmented cardiac cycles\n",
    "    label: corresponding class for each cc to generate activations w.r.t\n",
    "    \n",
    "    Outputs:\n",
    "    rec: concatenated cc\n",
    "    acti: concatenated CAMs\n",
    "    '''\n",
    "    \n",
    "    rec = []\n",
    "    activations = []\n",
    "    for idx,data in enumerate(cc):\n",
    "        if verbose:\n",
    "            print(\"Grad-CAM on CC-%d\" % idx)\n",
    "        data = np.expand_dims(data,axis=0)\n",
    "        \n",
    "        if output_class == 'true':\n",
    "            output = model.output[:,-(int(label[idx])+1)]\n",
    "        elif output_class == 'pred':\n",
    "            pred = np.argmax(model.predict(data,verbose=0),axis=-1)\n",
    "            output = model.output[:,-(int(pred)+1)]\n",
    "        else:\n",
    "            raise ValueError('output_class should be `true` or `pred`')\n",
    "        \n",
    "        last_conv_layer = model.get_layer(layer_name) ##### have to change the name here\n",
    "        grads = K.gradients(output, last_conv_layer.output)[0]\n",
    "        pooled_grads = K.mean(grads, axis=(0, 1)) ### no idea what to do here\n",
    "        iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "\n",
    "        pooled_grads_value, conv_layer_output_value = iterate([data])\n",
    "        for i in range(pooled_grads_value.shape[0]):\n",
    "            if verbose:\n",
    "                print(\"Iteration %d\" % i)\n",
    "            conv_layer_output_value[ :, i] *= pooled_grads_value[i]\n",
    "        heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "        if normalize:\n",
    "            print('normalizing')\n",
    "            try:\n",
    "                heatmap /= (np.std(heatmap)+ 1E-10)\n",
    "            except RuntimeWarning:\n",
    "                heatmap = heatmap\n",
    "        \n",
    "        \n",
    "        x = np.linspace(0, data.shape[1], num=len(heatmap))\n",
    "        y = heatmap\n",
    "        f1 = interp1d(x, y, kind='cubic')\n",
    "        xnew = np.linspace(0, data.shape[1], num=data.shape[1])\n",
    "        ynew = f1(xnew)\n",
    "        \n",
    "        end_idx = np.where(data!=0)[1][-1]\n",
    "        data = data[0,:end_idx,0]\n",
    "        ynew = ynew[:end_idx]\n",
    "        rec.append(data)\n",
    "        activations.append(ynew)\n",
    "    \n",
    "    return np.asarray(np.hstack(rec)),np.asarray(np.hstack(activations))\n",
    "\n",
    "def grad_cam_logs(logs,layer_name,cc,label,min_epoch=80,min_metric=.7,output_class='true',normalize=True,\n",
    "                  xlim=None,figsize=(12,8),lognames=None,colors=None,window='flat',win_size=50,\n",
    "                metric='val_macc',model_dir='../models/',verbose=0):\n",
    "    '''\n",
    "    Plot Grad_CAM for logs with predictions\n",
    "    '''\n",
    "    parser={0:'Normal',1:'Abnormal'}\n",
    "    \n",
    "    if not type(logs) == list:\n",
    "        logs = [logs]\n",
    "    \n",
    "    activations = []\n",
    "    predictions = []\n",
    "    for log_name in logs:\n",
    "        model = load_model(log_name,verbose=verbose)\n",
    "        weights = get_weights(log_name,min_epoch=min_epoch,\n",
    "                              min_metric=min_metric,verbose=verbose)\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "        if verbose:\n",
    "            print(\"GRAD CAM for %s\" % log_name)\n",
    "        _,acti = grad_cam_rec(model,layer_name,cc,label,\n",
    "                              verbose=verbose,\n",
    "                              normalize=normalize,\n",
    "                              output_class=output_class)\n",
    "        pred = cc2rec_labels(cc,model.predict(cc)[:,1])\n",
    "        activations.append(acti)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    rec = cc2rec(cc)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    grid = plt.GridSpec(3, 4, hspace=0.2, wspace=0.2,)\n",
    "    main_ax = fig.add_subplot(grid[0,:],\n",
    "                              ylabel='PCG',\n",
    "#                               ylabel='%s PCG'%parser[label[0]]\n",
    "                             )\n",
    "    pred_ax = fig.add_subplot(grid[1, :], ylabel='Predictions', sharex=main_ax)\n",
    "    acti_ax = fig.add_subplot(grid[2, :], ylabel='Activations', sharex=main_ax)\n",
    "    \n",
    "\n",
    "    t = np.linspace(0,len(rec)/1000,num=len(rec))\n",
    "    main_ax.plot(t,rec)\n",
    "    main_ax.set_xlim([0,t[-1]])\n",
    "    \n",
    "    if colors is not None:\n",
    "        for acti,pred,color in zip(activations,predictions,colors):\n",
    "            acti_ax.plot(t,smooth_win(acti/np.std(acti),window_len=win_size,window=window),color=color)\n",
    "            pred_ax.plot(t,pred,color=color)\n",
    "            \n",
    "    else:\n",
    "        for acti,pred in zip(activations,predictions):\n",
    "            acti_ax.plot(t,smooth_win(acti/np.std(acti),window_len=win_size,window=window))\n",
    "            pred_ax.plot(t,pred)\n",
    "    \n",
    "    pred_ax.set_ylim([0,1])\n",
    "    if xlim is not None:\n",
    "        main_ax.set_xlim(xlim)\n",
    "    if lognames is not None:\n",
    "        acti_ax.legend(lognames)\n",
    "        \n",
    "    return [main_ax,pred_ax,acti_ax]\n",
    "\n",
    "def smooth_win(x,window_len=11,window='hanning'):\n",
    "        if x.ndim != 1:\n",
    "                raise ValueError, \"smooth only accepts 1 dimension arrays.\"\n",
    "        if x.size < window_len:\n",
    "                raise ValueError, \"Input vector needs to be bigger than window size.\"\n",
    "        if window_len<3:\n",
    "                return x\n",
    "        if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "                raise ValueError, \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\"\n",
    "        s=np.r_[2*x[0]-x[window_len-1::-1],x,2*x[-1]-x[-1:-window_len:-1]]\n",
    "        if window == 'flat': #moving average\n",
    "                w=np.ones(window_len,'d')\n",
    "        else:  \n",
    "                w=eval('np.'+window+'(window_len)')\n",
    "        y=np.convolve(w/w.sum(),s,mode='same')\n",
    "        return y[window_len:-window_len+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "conf = model_confidence(model=model,data=x_val,labels=y_val, verbose=1)\n",
    "conf = cc2parts(conf,val_parts)\n",
    "plt.hist(conf,40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = np.logical_and(conf>.8,conf<.9)\n",
    "_,idx = np.where([cond])\n",
    "print('Number of Recordings within condition',len(idx))\n",
    "\n",
    "target_idx = np.random.randint(len(idx))\n",
    "print('Target Recording from subset-',cc2parts(val_files,val_parts)[idx[target_idx]])\n",
    "\n",
    "cc_idx = idx_parts2cc([idx[target_idx]],val_parts)\n",
    "\n",
    "target_data = x_val[cc_idx]\n",
    "target_labels = y_val[:,1][cc_idx]\n",
    "print('Target Recording Class',target_labels[0])\n",
    "target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Training Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,len(target_data)/1000,num=len(target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc2parts(train_files,train_parts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'a0182.wav'\n",
    "\n",
    "filenames = pd.read_csv('../data/feature/folds/text/train_files.txt',header=None)\n",
    "idx = np.where(filenames[0]==target)[0]\n",
    "print(idx)\n",
    "cc_idx = idx_parts2cc(idx,train_parts)\n",
    "target_data = x_train[cc_idx]\n",
    "target_labels = y_train[:,1][cc_idx]\n",
    "print('Target Recording Class',target_labels[0])\n",
    "print('Number of cc',target_data.shape[0])\n",
    "\n",
    "fig = plt.figure()\n",
    "rec = cc2rec(target_data[:6])\n",
    "plt.plot(np.linspace(0,len(rec)/1000,num=len(rec)),rec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Validation Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'b0003'\n",
    "filenames = pd.read_csv('../data/feature/folds/text/validation0.txt',header=None)\n",
    "idx = np.where(filenames[0]==target)[0]\n",
    "cc_idx = idx_parts2cc(idx,val_parts)\n",
    "\n",
    "target_data = x_val[cc_idx]\n",
    "target_labels = y_val[:,1][cc_idx]\n",
    "print('Target Recording Class',target_labels[0])\n",
    "print('Target Recording cc',target_labels.shape)\n",
    "target_data.shape\n",
    "\n",
    "fig = plt.figure()\n",
    "rec = cc2rec(target_data)\n",
    "plt.plot(np.linspace(0,len(rec)/1000,num=len(rec)),rec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Test Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 188\n",
    "cc_idx = idx_parts2cc(idx,test_parts)\n",
    "\n",
    "target_data = x_test[cc_idx]\n",
    "target_labels = y_test[:,1][cc_idx]\n",
    "print('Target Recording Class',target_labels[0])\n",
    "target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs=[\n",
    "\"potes_fold0_noFIR 2019-03-16 18:44:45.597226\", # potes non balanced\n",
    "\"potes_fold0_noFIR 2019-03-02 13:01:33.636778\", # potes\n",
    "\"fold0_noFIR 2019-02-27 19:52:21.543329\", # Type1 macc\n",
    "# \"fold0_noFIR 2019-03-07 14:44:47.022240\", # Type2 macc 80 epoch\n",
    "# \"fold0_noFIR 2019-03-08 03:28:46.740442\", # Type3 sensitivity\n",
    "# \"fold0_noFIR 2019-03-08 14:50:52.332924\", # type4 val_acc\n",
    "# \"fold0_noFIR 2019-03-09 01:34:03.547265\", # gamma stage 1\n",
    "\"fold0_noFIR 2019-03-06 14:21:29.823568\", # zero stage2\n",
    "]\n",
    "lognames=[\n",
    "\"Potes-CNN\",\n",
    "\"Potes-CNN DBT\",\n",
    "\"Type I tConv\",\n",
    "# \"Type II tConv\",\n",
    "# \"Type III tConv\",\n",
    "# \"Type IV tConv\",\n",
    "# \"Gammatone tConv\",\n",
    "\"Zero Phase tConv\",\n",
    "]\n",
    "colors = [\n",
    "'#434B77',\n",
    "'#669966',\n",
    "'#c10061',\n",
    "'#ff51a5',\n",
    "'k',\n",
    "'#ffbe4f',\n",
    "'#DBBBBB',\n",
    "'#008080',\n",
    "         ]\n",
    "\n",
    "cc_start = 0\n",
    "cc_end = 8\n",
    "ax = grad_cam_logs(logs,'concatenate_1',target_data[cc_start:cc_end],target_labels[cc_start:cc_end],win_size=10,\n",
    "                   lognames=lognames,colors=colors,output_class='pred',normalize=True)\n",
    "ax[1].set_yticks([0,.25,.5,.75,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(3.5,5)\n",
    "ax[1].set_yticks([0,.25,.5,.75,1])\n",
    "ax[2].set_ylim([-.1,6])\n",
    "# fig.savefig('Normal.eps')\n",
    "\n",
    "# plt.savefig('MRgradCAM.eps')\n",
    "# plt.xlim([0,4.7])\n",
    "\n",
    "ax[2].legend_ = None\n",
    "# ax[2].legend(lognames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_logs(logs,lognames,bins=5,figsize=(10,2),verbose=0):\n",
    "    fig,ax = plt.subplots(1,len(logs),sharey='row',figsize=figsize)\n",
    "    for axes,log_name,model_name in zip(ax,logs,lognames):\n",
    "        model = load_model(log_name,verbose=0)\n",
    "        weights = get_weights(log_name,min_epoch=100,min_metric=.7)\n",
    "        metric = 'val_macc'\n",
    "        model_dir = '../models/'\n",
    "        checkpoint_name = os.path.join(model_dir+log_name,weights[metric])\n",
    "        model.load_weights(checkpoint_name)\n",
    "\n",
    "        for subset in np.unique(val_files):\n",
    "            mask = np.asarray(val_files) == subset\n",
    "            part_mask = cc2parts(val_files,val_parts) == subset\n",
    "            conf = model_confidence(model=model,data=x_val[mask],labels=y_val[mask], verbose=verbose)\n",
    "            conf = cc2parts(conf,val_parts[part_mask])\n",
    "            sns.distplot(conf,bins=bins,label=\"Subset-%s\"%subset,ax=axes)\n",
    "\n",
    "        conf = model_confidence(model=model,data=x_test,labels=y_test, verbose=verbose)\n",
    "        conf = cc2parts(conf,test_parts)\n",
    "        sns.distplot(conf,bins=bins,label='HSSDB',ax=axes)\n",
    "        axes.set_title('%s C-DIST'%model_name)\n",
    "#         axes.legend(loc='upper center', bbox_to_anchor=(0.5, 1.00),&nbsp; shadow=True, ncol=2)\n",
    "    return ax\n",
    "ax = plot_confidence_logs(logs,lognames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chartBox = ax[2].get_position()\n",
    "# ax[0].set_position([chartBox.x0, chartBox.y0, chartBox.width*0.6, chartBox.height])\n",
    "ax[2].legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), shadow=True, ncol=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
